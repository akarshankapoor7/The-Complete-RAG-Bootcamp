{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a410300",
   "metadata": {},
   "source": [
    "### Semantic Chunking In RAG (Advanced Techinques)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf9bbe6",
   "metadata": {},
   "source": [
    "Semantic chunking means splitting a document into meaningful units based on the content/semantics, not just raw length or characters.\n",
    "\n",
    "Traditional chunking -> “cut every 500 tokens” (blind splitting).\n",
    "\n",
    "Semantic chunking -> “cut where topics or ideas change” (context-aware splitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac7904c",
   "metadata": {},
   "source": [
    "🔹 Benefits\n",
    "\n",
    "Preserves context → LLM sees complete ideas.\n",
    "\n",
    "Reduces hallucination → avoids half-cut concepts.\n",
    "\n",
    "Improves retrieval accuracy → relevant chunks match queries better.\n",
    "\n",
    "🔹 When to Use\n",
    "\n",
    "Long documents with multiple sections (research papers, legal docs, manuals).\n",
    "\n",
    "Knowledge bases where context boundaries matter.\n",
    "\n",
    "RAG pipelines where precise retrieval boosts performance.\n",
    "\n",
    "\n",
    "###############################################################\n",
    "\n",
    "✅ Summary in One Line\n",
    "Semantic chunking = intelligent splitting of documents into meaning-preserving chunks, unlike fixed-size chunking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48545b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# Access keys\n",
    "groq_api_key = os.getenv(\"GROQ_AI_API\")\n",
    "hf_api_key = os.getenv(\"HUGGINGFACE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "974d000b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1: LangChain is a powerful framework for building applications with Large Language Models (LLMs). It provides modular abstractions to connect and combine LLMs with external tools like OpenAI, Pinecone, and other data sources. Using LangChain, you can create pipelines that include chains, agents, memory modules, and retrievers to handle complex tasks.\n",
      "\n",
      "Chunk 2: The Eiffel Tower is located in Paris. France is widely known as a popular tourist destination.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "docs = \"\"\"LangChain is a powerful framework for building applications with Large Language Models (LLMs).\n",
    "It provides modular abstractions to connect and combine LLMs with external tools like OpenAI, Pinecone, and other data sources.\n",
    "Using LangChain, you can create pipelines that include chains, agents, memory modules, and retrievers to handle complex tasks.\n",
    "The Eiffel Tower is located in Paris.\n",
    "France is widely known as a popular tourist destination.\n",
    "\"\"\"\n",
    "\n",
    "# Split into sentences\n",
    "sentences = [s.strip() for s in docs.split(\"\\n\") if s.strip()]\n",
    "\n",
    "# Load embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "similarity_threshold = 0.31\n",
    "semantic_chunks = []\n",
    "\n",
    "# Initialize first chunk\n",
    "current_chunk = sentences[0]\n",
    "current_chunk_embeddings = [embeddings[0]]\n",
    "\n",
    "# Iterate through sentences\n",
    "for i in range(1, len(sentences)):\n",
    "    # Compute mean embedding of current chunk\n",
    "    chunk_mean = np.mean(current_chunk_embeddings, axis=0).reshape(1, -1)\n",
    "    sim = cosine_similarity(chunk_mean, embeddings[i].reshape(1, -1))[0][0]\n",
    "    \n",
    "    if sim >= similarity_threshold:\n",
    "        current_chunk += \" \" + sentences[i]\n",
    "        current_chunk_embeddings.append(embeddings[i])\n",
    "    else:\n",
    "        semantic_chunks.append(current_chunk)\n",
    "        current_chunk = sentences[i]\n",
    "        current_chunk_embeddings = [embeddings[i]]\n",
    "\n",
    "# Append the last chunk\n",
    "semantic_chunks.append(current_chunk)\n",
    "\n",
    "# Display chunks\n",
    "for idx, chunk in enumerate(semantic_chunks):\n",
    "    print(f\"Chunk {idx+1}: {chunk}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526e548b",
   "metadata": {},
   "source": [
    "### RAG pipeline modular coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2954acf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "from langchain.schema.runnable import RunnableLambda, RunnableMap\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# Access keys\n",
    "groq_api_key = os.getenv(\"GROQ_AI_API\")\n",
    "hf_api_key = os.getenv(\"HUGGINGFACE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80af733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Semantic Chunking for RAG Pipelines with thresholding\n",
    "\n",
    "class ThresholdSemanticChunker:\n",
    "    def __init__(self, model_name='all-MiniLM-L6-v2', similarity_threshold=0.31):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "\n",
    "    def chunk(self, documents):\n",
    "        sentences = [s.strip() for s in documents.split(\"\\n\") if s.strip()]\n",
    "        embeddings = self.model.encode(sentences)\n",
    "\n",
    "        semantic_chunks = []\n",
    "        current_chunk = sentences[0]\n",
    "        current_chunk_embeddings = [embeddings[0]]\n",
    "\n",
    "        for i in range(1, len(sentences)):\n",
    "            chunk_mean = np.mean(current_chunk_embeddings, axis=0).reshape(1, -1)\n",
    "            sim = cosine_similarity(chunk_mean, embeddings[i].reshape(1, -1))[0][0]\n",
    "\n",
    "            if sim >= self.similarity_threshold:\n",
    "                current_chunk += \" \" + sentences[i]\n",
    "                current_chunk_embeddings.append(embeddings[i])\n",
    "            else:\n",
    "                semantic_chunks.append(current_chunk)\n",
    "                current_chunk = sentences[i]\n",
    "                current_chunk_embeddings = [embeddings[i]]\n",
    "\n",
    "        semantic_chunks.append(current_chunk)\n",
    "        return semantic_chunks\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG ext",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
