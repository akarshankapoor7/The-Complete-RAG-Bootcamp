{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd534af3",
   "metadata": {},
   "source": [
    "*#### RAG stands for Retrieval Augmented Generation is a powerful technique that combines the capabiity of LLM with external knowlwdge retrieval.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f6bbc7",
   "metadata": {},
   "source": [
    "A typical RAG pipeline contains:\n",
    "\n",
    "\n",
    "âœ…Collect documents â†’ Load data (PDFs, Word, CSVs, APIs, etc.).\n",
    "\n",
    "âœ…Preprocess â†’ Clean, normalize, and enrich with metadata.\n",
    "\n",
    "âœ…Chunking â†’ Split into small overlapping pieces (e.g., 500â€“1000 tokens).\n",
    "\n",
    "âœ…Embedding â†’ Convert each chunk into dense vectors using an embedding model.\n",
    "\n",
    "âœ…Vector Store (Indexing) â†’ Store embeddings + metadata in a vector database (FAISS, Pinecone, Weaviate, Chroma, etc.).\n",
    "\n",
    "âœ…User Query â†’ Convert query into an embedding vector.\n",
    "\n",
    "âœ…Retrieval (via Vector Store) â†’ Search the vector store to get top-k most similar chunks.\n",
    "\n",
    "âœ…Augmentation â†’ Add retrieved chunks as context to the query.\n",
    "\n",
    "âœ…Generation â†’ Pass augmented input into an LLM (Groq, OpenAI, Anthropic, etc.) to generate the grounded answer.\n",
    "\n",
    "âœ…Post-processing â†’ Rerank, format, or cite sources from vector store metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66854019",
   "metadata": {},
   "source": [
    "\n",
    "*Collect â†’ Preprocess â†’ Chunk â†’ Embed â†’ Vector Store â†’ Query â†’ Retrieve â†’ Augment â†’ Generate â†’ Post-process*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfd357b",
   "metadata": {},
   "source": [
    "* Langchain: A framework for developing application powered by language models\n",
    "\n",
    "* Chroma DB: A open-source vector database for storing and retrieval embeddings\n",
    "\n",
    "* GROQ AI: A free LPI tool used for inferencing embedding and llm models efficiently and at free of cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213dcfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# Access keys\n",
    "groq_api_key = os.getenv(\"GROQ_AI_API\")\n",
    "hf_api_key = os.getenv(\"HUGGINGFACE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9da0b9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain imports\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# free llm and embedding models\n",
    "from langchain_groq import ChatGroq # for llm\n",
    "from langchain_huggingface import HuggingFaceEmbeddings # for embeddings\n",
    "\n",
    "# vector store\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# utility imports\n",
    "import numpy as np\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ee8139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents loaded: 3\n",
      "First document content: Artificial Intelligence (AI)\n",
      "\n",
      "Artificial Intelligence is the science and engineering of creating machines or systems that mimic human intelligence. It encompasses reasoning, problem-solving, decision-\n"
     ]
    }
   ],
   "source": [
    "#1 document loading\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "# loads document from directory\n",
    "\n",
    "loader =DirectoryLoader(\n",
    "    path=\"data/text_files\",\n",
    "    glob=\"*.txt\",\n",
    "    loader_cls=TextLoader,\n",
    "    loader_kwargs={\"encoding\":\"utf8\"}\n",
    ")\n",
    "\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"Number of documents loaded: {len(documents)}\")\n",
    "print(f\"First document content: {documents[0].page_content[0:200]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062e96c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created chunks 20 from 3 documents\n",
      "First chunk: Artificial Intelligence (AI)\n",
      "\n",
      "Artificial Intelligence is the science and engineering of creating machines or systems that mimic human intelligence. It encompasses reasoning, problem-solving, decision-\n",
      "Last chunk: As the ecosystem grows, LangChain continues to integrate with cutting-edge vector stores, model providers, and monitoring toolsâ€”making it the backbone of modern LLM application development.\n"
     ]
    }
   ],
   "source": [
    "#2 documents/ text splitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\",\". \", \"\",\"\\t\"]\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Created chunks {len(chunks)} from {len(documents)} documents\")  \n",
    "print(f\"First chunk: {chunks[0].page_content[0:200]}\")\n",
    "print(f\"Last chunk: {chunks[-1].page_content[0:200]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c1bd4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 Embedding Models\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Simple HuggingFace Embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"  # lightweight & fast\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "770cf753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created at: data/chroma_db\n",
      "Vector Store created with 20 vectors\n"
     ]
    }
   ],
   "source": [
    "# Set up Chroma vector store and index the document chunks\n",
    "\n",
    "persist_directory = \"data/chroma_db\"\n",
    "\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    persist_directory=persist_directory,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"RAG_collection\"\n",
    ")\n",
    "\n",
    "print(f\"Vector store created at: {persist_directory}\")\n",
    "print(f\"Vector Store created with {vector_store._collection.count()} vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496c43d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data\\\\text_files\\\\LangChain for LLM Applications.txt'}, page_content='As the ecosystem grows, LangChain continues to integrate with cutting-edge vector stores, model providers, and monitoring toolsâ€”making it the backbone of modern LLM application development.'),\n",
       " Document(metadata={'source': 'data\\\\text_files\\\\LangChain for LLM Applications.txt'}, page_content='ðŸ“˜ LangChain for LLM Applications\\nIntroduction to LangChain\\n\\nLangChain is an open-source framework designed to simplify the development of applications powered by Large Language Models (LLMs). It allows developers to connect LLMs with external data sources, APIs, and computational tools, enabling the creation of production-ready AI solutions.'),\n",
       " Document(metadata={'source': 'data\\\\text_files\\\\LangChain for LLM Applications.txt'}, page_content='Instead of interacting with LLMs in isolation, LangChain provides modular components that allow chaining of prompts, memory, agents, and external integrations. This makes it a preferred choice for building Retrieval-Augmented Generation (RAG), autonomous agents, chatbots, and data-driven applications.\\n\\nKey Components of LangChain\\n1. LLMs & Chat Models\\n\\nProvides interfaces to work with models like OpenAI GPT, Anthropic, Hugging Face, Groq, Cohere, and others.')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing my vector store - Basic level of similarity search\n",
    "query = \"Where do we use Langchain \"\n",
    "results = vector_store.similarity_search(query, k=3)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ddcb7c02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={'source': 'data\\\\text_files\\\\LangChain for LLM Applications.txt'}, page_content='As the ecosystem grows, LangChain continues to integrate with cutting-edge vector stores, model providers, and monitoring toolsâ€”making it the backbone of modern LLM application development.'),\n",
       "  0.7059235572814941),\n",
       " (Document(metadata={'source': 'data\\\\text_files\\\\LangChain for LLM Applications.txt'}, page_content='ðŸ“˜ LangChain for LLM Applications\\nIntroduction to LangChain\\n\\nLangChain is an open-source framework designed to simplify the development of applications powered by Large Language Models (LLMs). It allows developers to connect LLMs with external data sources, APIs, and computational tools, enabling the creation of production-ready AI solutions.'),\n",
       "  0.8082253932952881),\n",
       " (Document(metadata={'source': 'data\\\\text_files\\\\LangChain for LLM Applications.txt'}, page_content='Instead of interacting with LLMs in isolation, LangChain provides modular components that allow chaining of prompts, memory, agents, and external integrations. This makes it a preferred choice for building Retrieval-Augmented Generation (RAG), autonomous agents, chatbots, and data-driven applications.\\n\\nKey Components of LangChain\\n1. LLMs & Chat Models\\n\\nProvides interfaces to work with models like OpenAI GPT, Anthropic, Hugging Face, Groq, Cohere, and others.'),\n",
       "  0.9311261177062988)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Advanced Similarity Search with score\n",
    "\n",
    "query = \"Where do we use Langchain \"\n",
    "results = vector_store.similarity_search_with_score(query, k=3)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1817da",
   "metadata": {},
   "source": [
    "# Similarity Metrics in ChromaDB\n",
    "\n",
    "#### Understanding Similarity Score\n",
    "\n",
    "## L2 Distance (Euclidean Distance) - Default\n",
    "- **Definition**: Straight-line distance between vectors (L2 norm).\n",
    "- **Interpretation**: \n",
    "  - Lower scores = more similar.\n",
    "  - 0 = identical.\n",
    "  - Typical range: 0-2 (can be higher).\n",
    "- **Use Case**: Good for clustering or nearest-neighbor searches.\n",
    "\n",
    "## Cosine Similarity (If Configured)\n",
    "- **Definition**: Cosine of angle between vectors, ignores magnitude.\n",
    "- **Interpretation**: \n",
    "  - Higher scores = more similar.\n",
    "  - 1 = identical.\n",
    "  - Range: -1 to 1.\n",
    "- **Use Case**: Ideal for text analysis where direction matters more than length.\n",
    "\n",
    "## Key Differences\n",
    "- **L2**: Lower-is-better, sensitive to magnitude.\n",
    "- **Cosine**: Higher-is-better, direction-only.\n",
    "- ChromaDB defaults to L2; cosine is configurable.\n",
    "\n",
    "## Practical Notes\n",
    "- Choose metric based on data/task (e.g., cosine for text, L2 for images).\n",
    "- Outliers possible with high-dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff54c3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the LLM\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    api_key=groq_api_key,\n",
    "    temperature=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046a51f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akarshan Kapoor\\AppData\\Local\\Temp\\ipykernel_11184\\1858779618.py:1: LangChainDeprecationWarning: The method `BaseChatModel.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  test_response = llm.predict(\"How many states are in india including ut\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 28 states in India. \n",
      "\n",
      "However, if you are asking about the total number of union territories (UTs) in India, there are 8 union territories. \n",
      "\n",
      "So, the total number of states and union territories in India is 28 (states) + 8 (union territories) = 36.\n",
      "\n",
      "Here's the list of states and union territories in India:\n",
      "\n",
      "**States:**\n",
      "\n",
      "1. Andhra Pradesh\n",
      "2. Arunachal Pradesh\n",
      "3. Assam\n",
      "4. Bihar\n",
      "5. Chhattisgarh\n",
      "6. Goa\n",
      "7. Gujarat\n",
      "8. Haryana\n",
      "9. Himachal Pradesh\n",
      "10. Jharkhand\n",
      "11. Karnataka\n",
      "12. Kerala\n",
      "13. Madhya Pradesh\n",
      "14. Maharashtra\n",
      "15. Manipur\n",
      "16. Meghalaya\n",
      "17. Mizoram\n",
      "18. Nagaland\n",
      "19. Odisha\n",
      "20. Punjab\n",
      "21. Rajasthan\n",
      "22. Sikkim\n",
      "23. Tamil Nadu\n",
      "24. Telangana\n",
      "25. Tripura\n",
      "26. Uttar Pradesh\n",
      "27. Uttarakhand\n",
      "28. West Bengal\n",
      "\n",
      "**Union Territories:**\n",
      "\n",
      "1. Andaman and Nicobar Islands\n",
      "2. Chandigarh\n",
      "3. Dadra and Nagar Haveli and Daman and Diu\n",
      "4. Delhi\n",
      "5. Jammu and Kashmir\n",
      "6. Ladakh\n",
      "7. Lakshadweep\n",
      "8. Puducherry\n"
     ]
    }
   ],
   "source": [
    "# testing llm\n",
    "test_response = llm.predict(\"How many states are in india including ut\")\n",
    "print(test_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a8aaff",
   "metadata": {},
   "source": [
    "#### Modern  RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "96e691e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x00000239C08BB4D0>, search_kwargs={'k': 3})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert vector store to retriever\n",
    "\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_kwargs={\"k\": 3} #retrieve top 3 relevant chunks\n",
    ")\n",
    "\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d7958661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt=\"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Context: {context}\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a209d4ab",
   "metadata": {},
   "source": [
    "##### What is create_stuff_documents_chain?\n",
    "create_stuff_documents_chain creates a chain that \"stuffs\" (inserts) all retrieved documents into a single prompt and sends it to the LLM. It's called \"stuff\" because it literally stuffs all the documents into the context window at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d30ccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. \\nUse the following pieces of retrieved context to answer the question. \\nIf you don't know the answer, just say that you don't know. \\nUse three sentences maximum and keep the answer concise.\\n\\nContext: {context}\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x00000239ED279940>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000239ED27A270>, model_name='llama-3.1-8b-instant', temperature=0.2, model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Create a document chain\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "document_chain=create_stuff_documents_chain(llm,prompt)\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd05d353",
   "metadata": {},
   "source": [
    "This chain:\n",
    "\n",
    "- Takes retrieved documents\n",
    "- \"Stuffs\" them into the prompt's {context} placeholder\n",
    "- Sends the complete prompt to the LLM\n",
    "- Returns the LLM's response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81615192",
   "metadata": {},
   "source": [
    "#### What is create_retrieval_chain?\n",
    "create_retrieval_chain is a function that combines a retriever (which fetches relevant documents) with a document chain (which processes those documents with an LLM) to create a complete RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3cfccdde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x00000239C08BB4D0>, search_kwargs={'k': 3}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. \\nUse the following pieces of retrieved context to answer the question. \\nIf you don't know the answer, just say that you don't know. \\nUse three sentences maximum and keep the answer concise.\\n\\nContext: {context}\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "            | ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x00000239ED279940>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000239ED27A270>, model_name='llama-3.1-8b-instant', temperature=0.2, model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Create The Final RAG Chain\n",
    "\n",
    "from langchain.chains import create_retrieval_chain\n",
    "rag_chain=create_retrieval_chain(retriever,document_chain)\n",
    "rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58165665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What is Langchain?',\n",
       " 'context': [Document(metadata={'source': 'data\\\\text_files\\\\LangChain for LLM Applications.txt'}, page_content='ðŸ“˜ LangChain for LLM Applications\\nIntroduction to LangChain\\n\\nLangChain is an open-source framework designed to simplify the development of applications powered by Large Language Models (LLMs). It allows developers to connect LLMs with external data sources, APIs, and computational tools, enabling the creation of production-ready AI solutions.'),\n",
       "  Document(metadata={'source': 'data\\\\text_files\\\\LangChain for LLM Applications.txt'}, page_content='As the ecosystem grows, LangChain continues to integrate with cutting-edge vector stores, model providers, and monitoring toolsâ€”making it the backbone of modern LLM application development.'),\n",
       "  Document(metadata={'source': 'data\\\\text_files\\\\LangChain for LLM Applications.txt'}, page_content='Instead of interacting with LLMs in isolation, LangChain provides modular components that allow chaining of prompts, memory, agents, and external integrations. This makes it a preferred choice for building Retrieval-Augmented Generation (RAG), autonomous agents, chatbots, and data-driven applications.\\n\\nKey Components of LangChain\\n1. LLMs & Chat Models\\n\\nProvides interfaces to work with models like OpenAI GPT, Anthropic, Hugging Face, Groq, Cohere, and others.')],\n",
       " 'answer': 'LangChain is an open-source framework designed to simplify the development of applications powered by Large Language Models (LLMs). It connects LLMs with external data sources, APIs, and computational tools. This enables the creation of production-ready AI solutions.'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing the RAG chain for demo query\n",
    "response=rag_chain.invoke({\"input\":\"What is Langchain?\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "55ef07f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'What is Langchain?', 'context': [Document(metadata={'source': 'data\\\\text_files\\\\LangChain for LLM Applications.txt'}, page_content='ðŸ“˜ LangChain for LLM Applications\\nIntroduction to LangChain\\n\\nLangChain is an open-source framework designed to simplify the development of applications powered by Large Language Models (LLMs). It allows developers to connect LLMs with external data sources, APIs, and computational tools, enabling the creation of production-ready AI solutions.'), Document(metadata={'source': 'data\\\\text_files\\\\LangChain for LLM Applications.txt'}, page_content='As the ecosystem grows, LangChain continues to integrate with cutting-edge vector stores, model providers, and monitoring toolsâ€”making it the backbone of modern LLM application development.'), Document(metadata={'source': 'data\\\\text_files\\\\LangChain for LLM Applications.txt'}, page_content='Instead of interacting with LLMs in isolation, LangChain provides modular components that allow chaining of prompts, memory, agents, and external integrations. This makes it a preferred choice for building Retrieval-Augmented Generation (RAG), autonomous agents, chatbots, and data-driven applications.\\n\\nKey Components of LangChain\\n1. LLMs & Chat Models\\n\\nProvides interfaces to work with models like OpenAI GPT, Anthropic, Hugging Face, Groq, Cohere, and others.')], 'answer': 'LangChain is an open-source framework designed to simplify the development of applications powered by Large Language Models (LLMs). It connects LLMs with external data sources, APIs, and computational tools. This enables the creation of production-ready AI solutions.'}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6296c79d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain is an open-source framework designed to simplify the development of applications powered by Large Language Models (LLMs). It connects LLMs with external data sources, APIs, and computational tools. This enables the creation of production-ready AI solutions.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['answer']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a329ed89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are the three types of machine learning?\n",
      "--------------------------------------------------\n",
      "Answer: The three types of machine learning are: \n",
      "\n",
      "1. Supervised Learning: Uses labeled data to make predictions.\n",
      "2. Unsupervised Learning: Finds hidden structures in unlabeled data.\n",
      "3. Reinforcement Learning: Agents learn by trial-and-error using rewards and penalties.\n",
      "\n",
      "Retrieved Context:\n",
      "\n",
      "--- Source 1 ---\n",
      "Machine Learning (ML)\n",
      "\n",
      "Machine Learning is a subset of AI where systems learn from data and improve their performance automatically. Instead of being hard-coded with rules, ML models use algorithms to...\n",
      "\n",
      "--- Source 2 ---\n",
      "General AI (Strong AI): Hypothetical AI that could perform any intellectual task a human can do.\n",
      "\n",
      "Superintelligent AI: A future possibility where AI surpasses human intelligence.\n",
      "\n",
      "Applications: medica...\n",
      "\n",
      "--- Source 3 ---\n",
      "Reinforcement Learning: Agents learn by trial-and-error using rewards and penalties (e.g., AlphaGo).\n",
      "\n",
      "Applications: recommendation systems (Netflix, Amazon), predictive maintenance, stock market predi...\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Question: What is deep learning and how does it relate to neural networks?\n",
      "--------------------------------------------------\n",
      "Answer: Deep learning is a powerful branch of Machine Learning that uses artificial neural networks with multiple layers (deep architectures) to learn from massive amounts of data. It is modeled after the human brain's neurons, where information is processed and transmitted through interconnected layers.\n",
      "\n",
      "Retrieved Context:\n",
      "\n",
      "--- Source 1 ---\n",
      "Reinforcement Learning: Agents learn by trial-and-error using rewards and penalties (e.g., AlphaGo).\n",
      "\n",
      "Applications: recommendation systems (Netflix, Amazon), predictive maintenance, stock market predi...\n",
      "\n",
      "--- Source 2 ---\n",
      "Machine Learning (ML)\n",
      "\n",
      "Machine Learning is a subset of AI where systems learn from data and improve their performance automatically. Instead of being hard-coded with rules, ML models use algorithms to...\n",
      "\n",
      "--- Source 3 ---\n",
      "CNNs (Convolutional Neural Networks): Excellent for image recognition and computer vision.\n",
      "\n",
      "RNNs (Recurrent Neural Networks) & Transformers: Strong in sequence data like speech, text, and time series....\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Question: What are CNNs best used for?\n",
      "--------------------------------------------------\n",
      "Answer: CNNs (Convolutional Neural Networks) are excellent for image recognition and computer vision tasks. They are particularly useful for applications such as self-driving cars, healthcare (disease diagnosis from medical scans), and other tasks that involve analyzing visual data.\n",
      "\n",
      "Retrieved Context:\n",
      "\n",
      "--- Source 1 ---\n",
      "CNNs (Convolutional Neural Networks): Excellent for image recognition and computer vision.\n",
      "\n",
      "RNNs (Recurrent Neural Networks) & Transformers: Strong in sequence data like speech, text, and time series....\n",
      "\n",
      "--- Source 2 ---\n",
      "Reinforcement Learning: Agents learn by trial-and-error using rewards and penalties (e.g., AlphaGo).\n",
      "\n",
      "Applications: recommendation systems (Netflix, Amazon), predictive maintenance, stock market predi...\n",
      "\n",
      "--- Source 3 ---\n",
      "Recent advancements:\n",
      "\n",
      "Transformer models (BERT, GPT, T5): Revolutionized NLP with contextual understanding.\n",
      "\n",
      "Large Language Models (LLMs): Enable human-like conversation and reasoning.\n",
      "\n",
      "Multimodal AI:...\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to query the modern RAG system\n",
    "def query_rag_modern(question):\n",
    "    print(f\"Question: {question}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Using create_retrieval_chain approach\n",
    "    result = rag_chain.invoke({\"input\": question})\n",
    "    \n",
    "    print(f\"Answer: {result['answer']}\")\n",
    "    print(\"\\nRetrieved Context:\")\n",
    "    for i, doc in enumerate(result['context']):\n",
    "        print(f\"\\n--- Source {i+1} ---\")\n",
    "        print(doc.page_content[:200] + \"...\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test queries\n",
    "test_questions = [\n",
    "    \"What are the three types of machine learning?\",\n",
    "    \"What is deep learning and how does it relate to neural networks?\",\n",
    "    \"What are CNNs best used for?\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    result = query_rag_modern(question)\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e278f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG ext",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
